# Tokenizer Comparison

This project demonstrates the differences in tokenization between popular NLP models like:

- GPT-3.5 / GPT-4 (tiktoken)
- BERT (WordPiece)
- T5 (SentencePiece)
- Grok-1 (SentencePiece)

It also allows comparing token counts for different inputs.

## Files

- `app.py` : Streamlit application to input text and see tokenization results
- `requirements.txt` : Python dependencies
- `screenshots/` : Example screenshots from the app

## Screenshots
![Tokenizer Example](screenshots\image.png)
## How to Run

1. Clone the repo:
   

```bash
git clone <your-repo-url>
cd <repo-folder>


